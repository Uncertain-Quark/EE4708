{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification : Nearest Neighbors and Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Perform k-Nearest neighbours on the given dataset($X_{knn}$ and $y_{knn}$: where $X_{knn}$ stores feature vectors representing the movies and  $y_{knn}$ stores the 0-1 labelling for each movie) for binary classification of movies, for classifiying whether a given movie is a comedy(label 1) or not a comedy(label 0) . Split the dataset into train(80%), validation(10%) and test sets(10%).Run k-Nearest neighbours for different k values (1,3,7,15,31,63). Select the k, using validation set, which returns the best accuracy score. \n",
    "\n",
    "(i)  Report all the validation accuracies for all the values of k. \n",
    "<br>(ii) Report accuracy score by performing k-NN on the test dataset using the best chosen k value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of K : 1 corresponding validation Accuracy : 0.84\n",
      "Value of K : 3 corresponding validation Accuracy : 0.853\n",
      "Value of K : 7 corresponding validation Accuracy : 0.846\n",
      "Value of K : 15 corresponding validation Accuracy : 0.848\n",
      "Value of K : 31 corresponding validation Accuracy : 0.847\n",
      "Value of K : 63 corresponding validation Accuracy : 0.85\n"
     ]
    }
   ],
   "source": [
    "## write your code here.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = np.loadtxt('X_knn.csv')\n",
    "y = np.loadtxt('y_knn.csv')\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.1, random_state = 42)\n",
    "X_train , X_validation , y_train , y_validation = train_test_split(X_train,y_train,test_size = 1/9,random_state = 42)\n",
    "\n",
    "clf_list = []\n",
    "score_list = []\n",
    "for k in [1,3,7,15,31,63] :\n",
    "    clf = KNeighborsClassifier(k)\n",
    "    clf.fit(X_train,y_train)\n",
    "    score_list.append(clf.score(X_validation,y_validation))\n",
    "    clf_list.append(clf)\n",
    "\n",
    "for k,s in zip([1,3,7,15,31,63],score_list) :   \n",
    "    print('Value of K :',k,'corresponding validation Accuracy :',s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can see that from the validation accuracy, the value of 3 for k has the highest accuracy.\n",
      "For k value of 3, the accuracy on test is : 0.846\n"
     ]
    }
   ],
   "source": [
    "print('We can see that from the validation accuracy, the value of 3 for k has the highest accuracy.')\n",
    "print('For k value of 3, the accuracy on test is :',clf_list[1].score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the results here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) State why using an even value of k in k-NN should not be chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not choose even values of k for k nearest neighbours because if even values were chosen, there could arise a posibility wherein during testing a data point for classification, in the closest k train data points, half of them could be belonging to one class and half could belong to the other class. In this situation, we cannot decide which class the test point belongs to. Therefore, in order to avoid such cases, we do not use even values of k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Naive Bayes' classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Continuous Distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the distribution of the data( $X$ represents the datapoints and $Y$ represents the 0-1 binary-class label; where 0 being the negative class and 1 being the positive class) is already known.\n",
    "<br>Consider the following one-dimensional(1-D) Gaussian distributions where means and variances are unknown. You need to estimate means($\\mu_-$: for negative class and  $\\mu_+$: for positive class) and variances ($\\sigma^{2}_{-}$: for negative class and $\\sigma^{2}_+$: for positive class) from the given data : \n",
    "<br> (1) Assume $X|Y_{Y=0} \\sim \\mathcal{N}(\\mu_- , \\sigma^{2}_-)$ \n",
    "<br>(2) Assume $X|Y_{Y=1} \\sim \\mathcal{N}(\\mu_+ , \\sigma^{2}_+)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generating artificial datasets in the next cell *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is for generating datasets. Students should not change anything in this cell. \n",
    "## You can compare your mean and variance estimates by the actual ones used to generate these datasets\n",
    "\n",
    "import numpy as np\n",
    "X_pos = np.random.randn(1000,1)+np.array([[2.]])\n",
    "X_neg = np.random.randn(1000,1)+np.array([[4.]])\n",
    "X_train_pos = X_pos[:900]\n",
    "X_train_neg = X_neg[:900]\n",
    "X_test_pos = X_pos[900:]\n",
    "X_test_neg = X_neg[900:]\n",
    "X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)\n",
    "X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)\n",
    "Y_train = np.concatenate(( np.ones(900),np.zeros(900) ))\n",
    "Y_test = np.concatenate(( np.ones(100), np.zeros(100) ))\n",
    "\n",
    "## X_train, X_test, Y_train, Y_test are your datasets to work with ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>**Instructions to follow for learning a Baeysian classifier:** *(Code the formulae for estimating the different parameters yourself)*\n",
    "<br> a)Utilize the training dataset to estimate the means($\\hat{\\mu_+}$,$\\hat{\\mu_-}$) and variances($\\hat{\\sigma^{2}_+}$, $\\hat{\\sigma^{2}_-}$) for both positive and negative classes  \n",
    "b)Estimate the prior probability: $P(Y=1)$  ⟶ which could be referred to as: $\\hat{a}$ \n",
    "<br>c)Estimate the classifier funtion/posterior probability:  $P(Y=1|X = x)$  ⟶ which could be referred to as $\\hat{\\eta(x)}$\n",
    "<br>d)Find out the threshold value($x^*$) for classification by equating the estimated classifier function($\\hat{\\eta(x)}$)  with threshold probability of 0.5\n",
    "<br>e)Classify the test dataset into the two classes using this threshold value($x^*$) and find out the **accuracy** of the prediction \n",
    "\n",
    "Return back:  $\\hat{\\mu_+}$, $\\hat{\\mu_-}$, $\\hat{\\sigma^{2}_+}$, $\\hat{\\sigma^{2}_-}$, $\\hat{a}$, $x^*$ and accuracy from the code written \n",
    "\n",
    "*Hint: $X|Y_{Y=0} \\sim \\mathcal{N}(\\mu_- , \\sigma^{2}_-)$ implies $P_{X|Y=0} = \\mathcal{N}(\\mu_- , \\sigma^{2}_-) $*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_positive : 2.0261587372544527\n",
      "sigma_positive : 1.0299510787267079\n",
      "--------------------------------------\n",
      "mu_minus : 4.017737723461854\n",
      "sigma_negative : 0.9817377058426409\n",
      "--------------------------------------\n",
      "P(Y=1) : 0.5\n",
      "x* : 3.028077716033025\n",
      "Accuracy on Test Data : 0.83\n"
     ]
    }
   ],
   "source": [
    "## write your code here.  \n",
    "\n",
    "########## P(Y=1|X=x) = P(X=x|Y=1)/P(X=x)\n",
    "########## Here, we have took that P(X=x|Y=1) is a gaussian\n",
    "def eta_hat(x , mean_pos , var_pos , mean_neg , var_neg) :\n",
    "    numerator = a_hat * ( np.exp( -(x-mean_pos)**2/var_pos )/np.sqrt(2*np.pi*var_pos) )\n",
    "    denominator = a_hat * ( np.exp( -(x-mean_pos)**2/var_pos )/np.sqrt(2*np.pi*var_pos) ) + (1 - a_hat) * ( np.exp( -(x-mean_neg)**2/var_neg )/np.sqrt(2*np.pi*var_neg) )\n",
    "    p = numerator / denominator\n",
    "    return p\n",
    "########## Calculating the mean and variance using the fact that the observations for Y=1 and Y=0 are independent\n",
    "########## Hence, we can use law of large numbers and calculate the mean and corresponding variance for the train\n",
    "########## sample.\n",
    "mean_pos = np.sum(X_train[np.where(Y_train == 1)])/len(np.where(Y_train == 1)[0])\n",
    "var_pos =  np.sum( (X_train[np.where(Y_train == 1)] - mean_pos)**2 )/(len(np.where(Y_train == 1)[0]))\n",
    "\n",
    "mean_neg = np.sum(X_train[np.where(Y_train == 0)])/len(np.where(Y_train == 0)[0])\n",
    "var_neg =  np.sum( (X_train[np.where(Y_train == 0)] - mean_neg)**2 )/(len(np.where(Y_train == 0)[0]))\n",
    "\n",
    "########## P(Y=1) = total number of samples belonging to 1 / total number of train samples\n",
    "a_hat = len(np.where(Y_train == 1)[0])/Y_train.shape[0]\n",
    "\n",
    "########## To calculate x_star, we take a grid search on the x axis and calculate the eta_hat and take the x\n",
    "########## which gives the closest value to 0.5\n",
    "x = np.linspace( min(mean_pos,mean_neg) - 3*max(var_pos,var_neg),max(mean_pos,mean_neg) + 3*max(var_pos,var_neg),6000 )\n",
    "eta_list = []\n",
    "[eta_list.append(eta_hat(x[i],mean_pos,var_pos,mean_neg,var_neg)) for i in range(x.shape[0])]\n",
    "x_star = x[np.argmin(np.abs(np.array(eta_list) - 0.5))]\n",
    "\n",
    "Y_pred = []\n",
    "for i in range(X_test.shape[0]) :\n",
    "    if X_test[i] <= x_star :\n",
    "        Y_pred.append(1)\n",
    "    else :\n",
    "        Y_pred.append(0)\n",
    "Y_pred = np.array(Y_pred)        \n",
    "acc = np.sum(Y_pred == Y_test)/Y_test.shape[0]\n",
    "\n",
    "print('mu_positive :',mean_pos)\n",
    "print('sigma_positive :',var_pos)\n",
    "print('--------------------------------------')\n",
    "print('mu_minus :',mean_neg)\n",
    "print('sigma_negative :',var_neg)\n",
    "print('--------------------------------------')\n",
    "print('P(Y=1) :',a_hat)\n",
    "print('x* :',x_star)\n",
    "print('Accuracy on Test Data :',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results here\n",
    "\n",
    "As we can see, the observed mean and variance are close to the actual mean and variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Discrete distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the first exercise for learning the Naive Bayes' classifier where we dealt with continuous distribution of data, here you need to work with discrete data, which means finding Probability Mass Distribution(PMF). \n",
    "\n",
    "Age  | Income | Status  | Buy\n",
    "-----|--------|-------- |----\n",
    "<=20 |  low   | students| yes\n",
    "<=20 |  high  | students| yes\n",
    "<=20 | medium | students| no\n",
    "<=20 | medium | married | no\n",
    "<=20 |  high  | married | yes\n",
    "21-30|  low   | married | yes\n",
    "21-30|  low   | married | no \n",
    "21-30| medium | students| no\n",
    "21-30|  high  | students| yes\n",
    " >30 |  high  | married | no\n",
    " >30 |  high  | married | yes\n",
    " >30 | medium | married | yes\n",
    " >30 | medium | married | no\n",
    " >30 | medium | students| no\n",
    " \n",
    "Consider the train dataset above. Take any random datapoint ($X_{i}$) where $X_{i} = (X_{i,1} = Age,X_{i,2} = Income,X_{i,3} = Status)$ and its corresponding label \n",
    "\n",
    "($Y_{i} = Buy$). A \"yes\" in Buy corresponds to label-1 and a \"no\" in Buy corresponds to label-0.\n",
    "\n",
    "<br>**Instructions to follow for learning a Baeysian classifier:** *(Code the formulae for estimating the different parameters yourself)*\n",
    "<br> a)Estimate the prior probability: $P(Y=1)$  ⟶ which could be referred to as: $\\hat{a}$   \n",
    "b)Estimate the likelihood for each feature:  $P(X_{i,j} = x |Y = y_{i})$, where $ i$=datapoint counter, $j \\in \\{1,2,3\\}$ and $y_{i} \\in \\{0,1\\}$ \n",
    "<br>c)Estimate the total likelihood: $P(X_{i} = x |Y = y_{i})$  \n",
    "d)Calculate the posterior probability: $P(Y = 1|X_{i} = x_{test} )$ = $p_{test}$ where $x_{test} = (Age = 21-30, Income= medium, Status = married)$\n",
    "\n",
    "\n",
    "Return back: $\\hat{a}$, total likelihood and $p_{test}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dict_data = {'Age' : ['<=20','<=20','<=20','<=20','<=20','21-30','21-30','21-30','21-30','>30','>30','>30','>30','>30'],\n",
    "            'Income': ['low','high','medium','medium','high','low','low','medium','high','high','high','medium','medium','medium'],\n",
    "            'Status': ['students','students','students','married','married','married','married','students','students','married','married','married','married','students'],\n",
    "            'Buy' :['yes','yes','no','no','yes','yes','no','no','yes','no','yes','yes','no','no']}\n",
    "dataset = pd.DataFrame(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your code here.\n",
    "def prob(df , age=None,income=None,Status=None,Buy=None,conditional=True):\n",
    "    p = 1\n",
    "    if age is not None and conditional == True:\n",
    "        p = p * df.loc[df['Age'].isin(age) & df['Buy'].isin(Buy)].shape[0]/df.loc[df['Buy'].isin(Buy)].shape[0]\n",
    "    if income is not None and conditional == True :\n",
    "        p = p * (df.loc[df['Income'].isin(income) & df['Buy'].isin(Buy)]).shape[0]/df.loc[df['Buy'].isin(Buy)].shape[0]\n",
    "    if Status is not None and conditional == True :    \n",
    "        p = p * (df.loc[df['Status'].isin(Status) & df['Buy'].isin(Buy)]).shape[0]/df.loc[df['Buy'].isin(Buy)].shape[0]\n",
    "    if Buy is not None and conditional == False :\n",
    "        p = p * (df.loc[df['Buy'].isin(Buy)]).shape[0]/df.shape[0]\n",
    "    return p \n",
    "\n",
    "age_list = ['<=20','21-30','>30']\n",
    "age_dict = {'<=20':0 , '21-30' : 1 , '>30':2}\n",
    "income_list = ['low','medium','high']\n",
    "income_dict = {'low':0,'medium':1,'high':2}\n",
    "status_list = ['students','married']\n",
    "status_dict = {'students':0,'married':1}\n",
    "buy_list = ['yes','no']\n",
    "buy_dict = {'yes':1,'no':0}\n",
    "\n",
    "####### P(Y=1)\n",
    "a_hat = prob(dataset,Buy =['yes'],conditional=False)\n",
    "\n",
    "####### Calulating the individual feature probability for each feature vector\n",
    "prob_age = np.zeros((len(age_list),len(buy_list)))\n",
    "for i in age_list :\n",
    "    prob_age[age_dict[i],buy_dict['yes']] = prob(dataset,age=[i],Buy=['yes'])\n",
    "    prob_age[age_dict[i],buy_dict['no']] = prob(dataset,age=[i],Buy=['no'])\n",
    "\n",
    "prob_income = np.zeros((len(income_list),len(buy_list)))\n",
    "for i in income_list :\n",
    "    prob_income[income_dict[i],buy_dict['yes']] = prob(dataset,income=[i],Buy=['yes'])\n",
    "    prob_income[income_dict[i],buy_dict['no']] = prob(dataset,income=[i],Buy=['no'])\n",
    "\n",
    "prob_status = np.zeros((len(status_list),len(buy_list)))\n",
    "for i in status_list :\n",
    "    prob_status[status_dict[i],buy_dict['yes']] = prob(dataset,Status=[i],Buy=['yes'])\n",
    "    prob_status[status_dict[i],buy_dict['no']] = prob(dataset,Status=[i],Buy=['no'])\n",
    "\n",
    "####### Calculating the total likelihood making the assumption that the features are independent of each other\n",
    "####### Hence, we can multiply them\n",
    "prob_total_likelihood = np.zeros((len(age_list),len(income_list),len(status_list),len(buy_list)))\n",
    "for i in age_list :\n",
    "    for j in income_list : \n",
    "        for k in status_list :\n",
    "            for m in buy_list :\n",
    "                prob_total_likelihood[age_dict[i],income_dict[j],status_dict[k],buy_dict[m]] = prob(dataset,age=[i],Buy=[m])*prob(dataset,income=[j],Buy=[m])*prob(dataset,Status=[k],Buy=[m])\n",
    "\n",
    "\n",
    "####### calculating P(Y=1|X=x). \n",
    "####### We calculate this using Bayes Theorem P(Y=1|X=x) = P(X=x|Y=1)P(Y=1)/(P(X=x|Y=1)P(Y=1) + P(X=x|Y=0)P(Y=0))\n",
    "numerator = prob_total_likelihood[age_dict['21-30'],income_dict['medium'],status_dict['married'],buy_dict['yes']] * a_hat\n",
    "denomenator = prob_total_likelihood[age_dict['21-30'],income_dict['medium'],status_dict['married'],buy_dict['no']]*(1-a_hat) + prob_total_likelihood[age_dict['21-30'],income_dict['medium'],status_dict['married'],buy_dict['yes']] * a_hat\n",
    "p_test = numerator/denomenator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y=1) : 0.5\n"
     ]
    }
   ],
   "source": [
    "print('P(Y=1) :',a_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p test : 0.16666666666666663\n"
     ]
    }
   ],
   "source": [
    "print('p test :',p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Likelihood :\n",
      "[[[[0.01749271 0.05247813]\n",
      "   [0.02332362 0.06997085]]\n",
      "\n",
      "  [[0.08746356 0.02623907]\n",
      "   [0.11661808 0.03498542]]\n",
      "\n",
      "  [[0.01749271 0.10495627]\n",
      "   [0.02332362 0.13994169]]]\n",
      "\n",
      "\n",
      " [[[0.01749271 0.03498542]\n",
      "   [0.02332362 0.04664723]]\n",
      "\n",
      "  [[0.08746356 0.01749271]\n",
      "   [0.11661808 0.02332362]]\n",
      "\n",
      "  [[0.01749271 0.06997085]\n",
      "   [0.02332362 0.09329446]]]\n",
      "\n",
      "\n",
      " [[[0.02623907 0.03498542]\n",
      "   [0.03498542 0.04664723]]\n",
      "\n",
      "  [[0.13119534 0.01749271]\n",
      "   [0.17492711 0.02332362]]\n",
      "\n",
      "  [[0.02623907 0.06997085]\n",
      "   [0.03498542 0.09329446]]]]\n",
      "Axis : 0 - Age , 1 - Income , 2 - Status , 3 - Buy\n"
     ]
    }
   ],
   "source": [
    "print('Total Likelihood :')\n",
    "print(prob_total_likelihood)\n",
    "print('Axis : 0 - Age , 1 - Income , 2 - Status , 3 - Buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
